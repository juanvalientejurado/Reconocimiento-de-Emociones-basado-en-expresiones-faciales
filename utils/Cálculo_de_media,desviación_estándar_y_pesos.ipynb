{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cálculo de media,desviación estándar y pesos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ho34sEB81bMM"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#Librerías necesarias\n",
        "######################\n",
        "\n",
        "#Librerías de python y google colab\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import random\n",
        "import copy\n",
        "import itertools\n",
        "from google.colab import drive\n",
        "\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Pytorch\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.transforms.transforms import FiveCrop\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import sampler, random_split\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torchsummary import summary\n",
        "\n",
        "#Scikit Learn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Asociar la cuenta de Google Drive con el cuaderno para leer los archivos .tar\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4JHwKXe8LKMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3796a05-0589-48c5-9c4b-9efc77919318"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracción de los archivos .tar\n",
        "\n",
        "data_dir = \"/content/data\"\n",
        "\n",
        "#Extracción de train (cambiar directorio)\n",
        "tar = tarfile.open(\"/content/drive/MyDrive/TFG/train_set.tar\")\n",
        "tar.extractall(data_dir)\n",
        "tar.close()\n",
        "\n",
        "#Extraemos test (cambiar directorio)\n",
        "tar = tarfile.open(\"/content/drive/MyDrive/TFG/val_set.tar\")\n",
        "tar.extractall(data_dir)\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "o6-CfEYdLLpc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cálculo de la media y desviación estándar"
      ],
      "metadata": {
        "id": "e9WwKVf6PKL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función que calcula la media y la desviación estándar \n",
        "\n",
        "def media_std(loader):\n",
        "\n",
        "  #VAR[x] = E[X**2] - E[x]**2\n",
        "  suma_canales, sumacuadradocanales, num_batches = 0,0,0\n",
        "\n",
        "  for i, (data, target) in enumerate(loader):\n",
        "    \n",
        "    suma_canales += torch.mean(data, dim=[0,2,3])#Media para el ancho y alto\n",
        "    sumacuadradocanales += torch.mean(data**2, dim =[0,2,3]) #E[x**2]\n",
        "    \n",
        "    num_batches +=1\n",
        "  \n",
        "  #Dividimos la suma entre el número de batches para obtener la media\n",
        "  media = suma_canales/num_batches\n",
        "  #Hacemos la raíz cuadrada de la varianza para obtener la desviación estándar\n",
        "  std = (sumacuadradocanales/num_batches - media**2)**0.5\n",
        "\n",
        "  return media, std\n"
      ],
      "metadata": {
        "id": "6yZusYcNLMh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clase AffectNet para el conjunto de test\n",
        "\n",
        "class AffectNetDataset_test(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform = None):\n",
        "\n",
        "    self.annotations= pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    img_path=os.path.join(self.root_dir, self.annotations.iloc[index,0])\n",
        "    image = io.imread(img_path)\n",
        "    y_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
        "    \n",
        "    img = self.transform(image)\n",
        "    \n",
        "    return (img, y_label)"
      ],
      "metadata": {
        "id": "fXBr9WiuNs5f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "train_dir= \"/content/data/train_set/images\"\n",
        "\n",
        "#Creamos un \"dumb\" dataset que se usa para obtener la media  y la std\n",
        "dumb_dataset = AffectNetDataset_test(csv_file='AffectNet.csv', root_dir = train_dir, transform = transforms.ToTensor())\n",
        "dumb_loader = DataLoader(dataset = dumb_dataset, batch_size = 256, shuffle = True)\n",
        "\n",
        "mean,std = media_std(dumb_loader)\n",
        "print(mean)\n",
        "print(std)"
      ],
      "metadata": {
        "id": "Z2CLxCVwLV_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f9c389-6c33-4876-fd59-e8e07d2d36cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5694, 0.4460, 0.3912])\n",
            "tensor([0.2747, 0.2446, 0.2383])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo de los pesos"
      ],
      "metadata": {
        "id": "u5N8mN5IO5NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Para el cáclulo de los pesos, es necesario disponer de las etiquetas sin crear una base de datos\n",
        "\n",
        "train_annotations_path = \"/content/data/train_set/annotations\"\n",
        "train_dir = os.listdir(train_annotations_path)\n",
        "val_annotations_path = \"/content/data/val_set/annotations\"\n",
        "dir_val = os.listdir(val_annotations_path)\n",
        "\n",
        "\n",
        "#Si el archivo contiene exp.npy, metemos su ruta en el array \n",
        "train_npy=[]\n",
        "val_npy=[]\n",
        "\n",
        "for i in range(len(train_dir)):\n",
        "  if 'exp.npy' in train_dir[i]:\n",
        "    train_npy.append(train_dir[i])\n",
        "\n",
        "for i in range(len(dir_val)):\n",
        "  if 'exp.npy' in dir_val[i]:\n",
        "    val_npy.append(dir_val[i])\n",
        "\n",
        "#Ordenamos el array mirando su primer elemento\n",
        "train_npy.sort(key=lambda x: [int(x[0:-8])])\n",
        "print(len(train_npy))\n",
        "\n",
        "val_npy.sort(key=lambda x: [int(x[0:-8])])\n",
        "print(len(val_npy))\n",
        "\n",
        "\n",
        "#Función que procesa el archivo npy y obtiene su valor\n",
        "def npy_loader(path):\n",
        "   sample = np.load(path)\n",
        "   return sample\n",
        "\n",
        "#Guardamos las etiquetas en el array clases\n",
        "train_classes=[]\n",
        "for item in range(len(train_npy)):\n",
        "  train_classes.append(npy_loader(train_annotations_path + \"/\" + train_npy[item]))\n",
        "\n",
        "val_classes=[]\n",
        "for item in range(len(val_npy)):\n",
        "  #print(item)\n",
        "  val_classes.append(npy_loader(val_annotations_path + \"/\" + val_npy[item]))\n"
      ],
      "metadata": {
        "id": "bKSq732KPO3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b232308a-e9c7-4b6e-e147-2910e96282db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287651\n",
            "3999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se procede de la misma forma con las imágenes\n",
        "\n",
        "train_images_path = \"/content/data/train_set/images\"\n",
        "train_images = os.listdir(train_images_path)\n",
        "train_images.sort(key=lambda x: [int(x[0:-4])])\n",
        "\n",
        "val_images_path = \"/content/data/val_set/images\"\n",
        "val_images = os.listdir(val_images_path)\n",
        "val_images.sort(key=lambda x: [int(x[0:-4])])"
      ],
      "metadata": {
        "id": "6mbCpdooPRB-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creación del archivo CSV que contiene la ruta a las imágenes y su etiqueta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Num_total_train=len(train_images)\n",
        "\n",
        "\n",
        "#Guardamos el nombre del archivo y su etiqueta en un dataframe de pandas\n",
        "data={}\n",
        "data['images']= train_images\n",
        "data['label']= train_classes\n",
        "\n",
        "print(len(data['images']))\n",
        "print(len(data['label']))\n",
        "data_df = pd.DataFrame(data, columns=['images', 'label'])\n",
        "print(data_df)\n",
        "\n",
        "data_df.to_csv(\"AffectNet.csv\", index=False, header = False)\n",
        "\n",
        "\n",
        "val_data={}\n",
        "val_data['images']= val_images\n",
        "val_data['label']= val_classes\n",
        "\n",
        "val_df = pd.DataFrame(val_data, columns=['images', 'label'])\n",
        "print(val_df)\n",
        "\n",
        "val_df.to_csv(\"AffectNet_val.csv\", index=False, header = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FdCK7XjQPT9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0358f02-9c04-494e-dfab-30c1a17fee91"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287651\n",
            "287651\n",
            "            images label\n",
            "0            0.jpg     1\n",
            "1            1.jpg     0\n",
            "2            2.jpg     0\n",
            "3            3.jpg     1\n",
            "4            5.jpg     6\n",
            "...            ...   ...\n",
            "287646  414792.jpg     2\n",
            "287647  414793.jpg     0\n",
            "287648  414794.jpg     2\n",
            "287649  414795.jpg     2\n",
            "287650  414796.jpg     1\n",
            "\n",
            "[287651 rows x 2 columns]\n",
            "        images label\n",
            "0        0.jpg     0\n",
            "1        1.jpg     0\n",
            "2        2.jpg     4\n",
            "3        3.jpg     0\n",
            "4        4.jpg     2\n",
            "...        ...   ...\n",
            "3994  5489.jpg     0\n",
            "3995  5490.jpg     6\n",
            "3996  5492.jpg     6\n",
            "3997  5494.jpg     3\n",
            "3998  5495.jpg     2\n",
            "\n",
            "[3999 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Array que contiene las etiquetas \n",
        "\n",
        "train_labels= []\n",
        "\n",
        "#Guardamso las etiquetas en un array\n",
        "for idx, (data, label) in enumerate(dumb_dataset):\n",
        "  train_labels.append(label.item())\n",
        "\n",
        "#Creamos una serie que cuenta las apariciones de cada etiqueta \n",
        "train_counts = pd.Series(train_labels).value_counts().sort_index()\n",
        "print(train_counts)\n",
        "\n",
        "#Calculamos los pesos como la inversa\n",
        "train_weights= (1/torch.Tensor(train_counts))\n",
        "print(train_weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "VrBGhO-jOMMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3c215f-88e4-452f-826e-3c045b33acc7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     74874\n",
            "1    134414\n",
            "2     25459\n",
            "3     14090\n",
            "4      6378\n",
            "5      3803\n",
            "6     24882\n",
            "7      3750\n",
            "dtype: int64\n",
            "tensor([1.3356e-05, 7.4397e-06, 3.9279e-05, 7.0972e-05, 1.5679e-04, 2.6295e-04,\n",
            "        4.0190e-05, 2.6667e-04])\n"
          ]
        }
      ]
    }
  ]
}